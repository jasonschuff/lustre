# -*- mode: c; c-basic-offset: 4; indent-tabs-mode: nil; -*-
# vim:expandtab:shiftwidth=4:tabstop=4:

General {
	fs_path = __MOUNT_POINT__;
	fs_type = lustre;
	stay_in_fs = yes;
    check_mounted = yes;
    last_access_only_atime = no;
    uid_gid_as_numbers = no;
}


# updt params configuration
db_update_params
{
	# possible policies for refreshing metadata and path in database:
	#   never: get the information once, then never refresh it
	#   always: always update entry info when processing it
	#   on_event: only update on related event
	#   periodic(interval): only update periodically
	#   on_event_periodic(min_interval,max_interval)= on_event + periodic
	# Updating of file metadata
	md_update = always ;
	# Updating file path in database
	path_update = on_event_periodic(0,1h) ;
	# File classes matching
	fileclass_update = always ;
}

# entry processor configuration
EntryProcessor
{
	# nbr of worker threads for processing pipeline tasks
	nb_threads = 16 ;
	
	# Max number of operations in the Entry Processor pipeline.
	# If the number of pending operations exceeds this limit, 
	# info collectors are suspended until this count decreases
	max_pending_operations = 100 ;
	
	# max batched DB operations (1=no batching)
	max_batch_size = 100;
	
	# Optionnaly specify a maximum thread count for each stage of the pipeline:
	# <stagename>_threads_max = <n> (0: use default)
	# STAGE_GET_FID_threads_max = 4 ;
	# STAGE_GET_INFO_DB_threads_max     = 4 ;
	# STAGE_GET_INFO_FS_threads_max     = 4 ;
	# STAGE_PRE_APPLY_threads_max       = 4 ;
	# Disable batching (max_batch_size=1) or accounting (accounting=no)
	# to allow parallelizing the following step:
	# STAGE_DB_APPLY_threads_max        = 4 ;
	# if set to 'no', classes will only be matched
	# at policy application time (not during a scan or reading changelog)
	match_classes = yes;
	
	# Faking mtime to an old time causes the file to be migrated
	# with top priority. Enabling this parameter detect this behavior
	# and doesn't allow  mtime < creation_time
	detect_fake_mtime = no;
}

# FS scan configuration
FS_Scan
{
	# simple scan interval (fixed)
	scan_interval      =   2d ;
	
	# min/max for adaptive scan interval:
	# the more the filesystem is full, the more frequently it is scanned.
	#min_scan_interval      =   24h ;
	#max_scan_interval      =    7d ;
	# number of threads used for scanning the filesystem
	nb_threads_scan        =     2 ;
	
	# when a scan fails, this is the delay before retrying
	scan_retry_delay       =    1h ;
	
	# timeout for operations on the filesystem
	scan_op_timeout        =    1h ;
	
	# exit if operation timeout is reached?
	exit_on_timeout        =    yes ;
	
	# external command called on scan termination
	# special arguments can be specified: {cfg} = config file path,
	# {fspath} = path to managed filesystem
	#completion_command     =    "/path/to/my/script.sh -f {cfg} -p {fspath}" ;
	# Internal scheduler granularity (for testing and of scan, hangs, ...)
	spooler_check_interval =  1min ;
	
	# Memory preallocation parameters
	nb_prealloc_tasks      =   256 ;
	Ignore
	{
		# ignore ".snapshot" and ".snapdir" directories (don't scan them)
		type == directory
		and
		( name == ".snapdir" or name == ".snapshot" )
	}
}

#### policy definitions ####

# include template policy definitions for Lustre/HSM
%include "includes/lhsm.inc"
__POLICY_FILE__


########### end of policy rules ############

# changelog reader configuration
# Parameters for processing MDT changelogs :
ChangeLog
{
	# 1 MDT block for each MDT :
	MDT
	{
		# name of the first MDT
		mdt_name  = "MDT0000" ;
		
		# id of the persistent changelog reader
		# as returned by "lctl changelog_register" command
		reader_id = "cl1" ;
	}
	# clear changelog every 1024 records:
	batch_ack_count = 1024 ;
	force_polling    = yes ;
	polling_interval = 1s ;
	
	# changelog batching parameters
	queue_max_size   = 1000 ;
	queue_max_age    = 5s ;
	queue_check_interval = 1s ;
	
	# delays to update last committed record in the DB
	commit_update_max_delay = 5s ;
	commit_update_max_delta = 10k ;
	
	# uncomment to dump all changelog records to the file
}
		


# logs configuration
Log
{
	# log levels: CRIT, MAJOR, EVENT, VERB, DEBUG, FULL
	debug_level = EVENT;
	
	# Log file
	log_file = "/var/log/robinhood.log";
	
	# File for reporting purge events
	report_file = "/var/log/robinhood_actions.log";
	alert_file = "/var/log/robinhood_alerts.log";
	changelogs_file = "/var/log/robinhood_cl.log";
	stats_interval = 5min;
	batch_alert_max = 5000;
	alert_show_attrs = yes;
	log_procname = yes;
	log_hostname = yes;
}

ListManager {

	# Method for committing information to database.
	# Possible values are:
	# - "autocommit": weak transactions (more efficient, but database inconsistencies may occur)
	# - "transaction": manage operations in transactions (best consistency, lower performance)
	# - "periodic(<nb_transaction>)": periodically commit (every <n> transactions).
	commit_behavior = transaction ;
	
	# Minimum time (in seconds) to wait before trying to reestablish a lost connection.
	# Then this time is multiplied by 2 until reaching connect_retry_interval_max
	connect_retry_interval_min = 1 ;
	connect_retry_interval_max = 30 ;
	
	# disable the following options if you are not interested in
	# user or group stats (to speed up scan)
	accounting  = enabled ;

	MySQL {
		server = "localhost";
		db = "__DB_NAME__";
        user = "robinhood";
		# password or password_file are mandatory
		password_file = "/etc/robinhood.d/.dbpassword" ;
        engine = innodb;
	}
}


